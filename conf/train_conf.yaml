# =========================
# Experiment configurations
# =========================
defaults:
  - loggers:
    - tensorboard
  - /callbacks:
    - lr_monitor
    - grad_norm
    - speed_monitor
    - model_checkpoint
    - prediction_writer
    # - early_stopping
  - model: picodeeplob
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog
  - _self_


hydra:
  # adds colorlog to logging
  job_logging:
    formatters:
      colorlog:
        format: '[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s] - %(message)s'
        log_colors:
          DEBUG: purple
          INFO: green
          WARNING: yellow
          ERROR: red
          CRITICAL: bold_red
  # tells hydra to make the run/dir the current working director
  job:
    chdir: true
  # folder structure
  run:
    dir: ./outputs/${out_parent_folder}/${run_folder}
  sweep:
    dir: ./outputs/multirun/${out_parent_folder}
    subdir: ${run_folder}_${hydra.job.id}


# Convenience
run_folder: ${now:%Y-%m-%d}T${now:%H-%M-%S}
out_parent_folder: model_train
seed: 42


# Data
repo_path: /home/pl487/time-series-prediction

datamodule:
  window_size: 100
  use_prev_y: false
  train_data_path: ${repo_path}/data/train_memmap.npy
  val_data_path: ${repo_path}/data/val_memmap.npy
  test_data_path: ${repo_path}/data/test_memmap.npy

data:
  batch_size: 64
  eval_batch_size: 128
  shuffle: true
  drop_last: true
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  multiprocessing_context: null


# Model
resume_from_checkpoint: null
save_initial_checkpoint: true
torch_compile: false


optim:
  optim_name: adamw
  lr: 5e-3
  weight_decay: 0.1
  optim_kwargs:
    fused: true
    eps: 1e-8
    betas: [0.9, 0.95]
  scheduler_name: cosine_with_warmup
  # scheduler_kwargs:
  #   num_cycles: 40  # max_epochs * 4
  num_warmup_steps: 2000


trainer:
  accelerator: gpu
  precision: 32-true
  deterministic: false
  log_every_n_steps: 1
  enable_progress_bar: true
  fast_dev_run: false
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  val_check_interval: 0.25
  max_epochs: 10
  # max_epochs: 1
  # limit_train_batches: 10
  # limit_val_batches: 10
  # limit_test_batches: 10

